{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OZuiCxPuu_hJGb3lGuAm2R7t9_ThBf0b",
      "authorship_tag": "ABX9TyOmVm8jZJlqfO5DEK3c51Ks",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubaidillah-chem/fouling-ml/blob/main/07_mlp_visualizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fgLWBJQ34fr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle, Arrow, FancyBboxPatch\n",
        "from typing import Dict, List, Tuple, Union\n",
        "import re\n",
        "\n",
        "def visualize_mlp(model_repr: str, config: Dict = None):\n",
        "    \"\"\"Visualize an MLP architecture from a PyTorch model's string representation.\n",
        "\n",
        "    Args:\n",
        "        model_repr: String representation of a PyTorch model (from repr(model))\n",
        "        config: Dictionary containing visualization parameters\n",
        "    \"\"\"\n",
        "    # Default configuration\n",
        "    default_config = {\n",
        "        # Figure parameters\n",
        "        'figsize': (14*5, 6*5),\n",
        "        'x_limits': (0, 18*5),\n",
        "        'y_limits': (0, 6*5),\n",
        "\n",
        "        # Layer box parameters\n",
        "        'box_width': 1.0*5,\n",
        "        'box_height': 0.5*5,\n",
        "        'box_rounding': 0.1*5,\n",
        "        'box_padding': 0.05*5,\n",
        "        'box_linewidth': 1.5*5,\n",
        "\n",
        "        # Spacing parameters\n",
        "        'layer_spacing': 1.6*5,\n",
        "        'start_x': 1*5,\n",
        "        'layer_y': 3*5,\n",
        "\n",
        "        # Arrow parameters\n",
        "        'arrow_width': 0.08*5,\n",
        "        'arrow_color': '#555555',\n",
        "        'arrow_alpha': 0.7,\n",
        "\n",
        "        # Group box parameters\n",
        "        'group_box_height': 0.9*5,\n",
        "        'group_box_rounding': 0.2*5,\n",
        "        'group_box_padding': 0.15*5,\n",
        "        'group_box_linewidth': 2*5,\n",
        "        'group_box_alpha': 0.6,\n",
        "\n",
        "        # Text parameters\n",
        "        'fontsize': 10*5,\n",
        "        'title_fontsize': 14*5,\n",
        "\n",
        "        # Colors\n",
        "        'colors': {\n",
        "            'Input': '#FF9AA2',\n",
        "            'Linear': '#A2E1FF',\n",
        "            'ReLU': '#B5EAD7',\n",
        "            'BatchNorm': '#FFDAC1',\n",
        "            'Output': '#C7CEEA',\n",
        "            'Other': '#E2F0CB'\n",
        "        },\n",
        "\n",
        "        # Title\n",
        "        'title': \"MLP Architecture Visualization\",\n",
        "\n",
        "        # Rename layer type\n",
        "        'rename': {\n",
        "            'BatchNorm1d': 'BatchNorm'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Merge user config with defaults\n",
        "    if config:\n",
        "        default_config.update(config)\n",
        "    config = default_config\n",
        "\n",
        "    def parse_model_repr(repr_str: str) -> List[Tuple]:\n",
        "        \"\"\"Parse the model representation string into layer information.\"\"\"\n",
        "        layers = []\n",
        "\n",
        "        # Extract the Sequential block if present\n",
        "        sequential_match = re.search(r'Sequential\\(([\\s\\S]*?)\\)\\)', repr_str)\n",
        "        if sequential_match:\n",
        "            content = sequential_match.group(1)\n",
        "        else:\n",
        "            content = repr_str\n",
        "\n",
        "        # Find all layer lines with their parameters\n",
        "        layer_lines = re.findall(r'\\(\\d+\\): (\\w+)\\(([\\s\\S]*?)(?=\\)\\n|$)', content)\n",
        "\n",
        "        # First pass: identify all linear layers to find the last one\n",
        "        linear_indices = [i for i, (layer_type, _) in enumerate(layer_lines)\n",
        "                         if layer_type.strip() == 'Linear']\n",
        "        last_linear_idx = linear_indices[-1] if linear_indices else -1\n",
        "\n",
        "        for i, (layer_type, params) in enumerate(layer_lines):\n",
        "            layer_type = layer_type.strip()\n",
        "            layer_type = config['rename'].get(layer_type, layer_type)\n",
        "\n",
        "            if layer_type == 'Linear':\n",
        "                # Extract in_features and out_features\n",
        "                in_match = re.search(r'in_features=(\\d+)', params)\n",
        "                out_match = re.search(r'out_features=(\\d+)', params)\n",
        "                in_feat = int(in_match.group(1)) if in_match else 0\n",
        "                out_feat = int(out_match.group(1)) if out_match else 0\n",
        "\n",
        "                # Check if this is the last linear layer\n",
        "                if i == last_linear_idx:\n",
        "                    layers.append((f\"Output\\n({out_feat})\",\n",
        "                                 in_feat, out_feat,\n",
        "                                 config['colors']['Output']))\n",
        "                else:\n",
        "                    layers.append((f\"{layer_type}\\n({out_feat})\",\n",
        "                                 in_feat, out_feat,\n",
        "                                 config['colors']['Linear']))\n",
        "            elif layer_type == 'ReLU':\n",
        "                prev_out = layers[-1][2] if layers else 0\n",
        "                layers.append((layer_type, prev_out, prev_out,\n",
        "                             config['colors']['ReLU']))\n",
        "            elif layer_type == 'BatchNorm1d':\n",
        "                num_match = re.search(r'(num_features=(\\d+))|(\\d+)(?=\\s*,)', params)\n",
        "                if num_match:\n",
        "                    num_feat = int(num_match.group(2)) if num_match.group(2) else int(num_match.group(3))\n",
        "                else:\n",
        "                    num_feat = layers[-1][2] if layers else 0\n",
        "                layers.append((f\"{layer_type}\\n({num_feat})\",\n",
        "                             num_feat, num_feat,\n",
        "                             config['colors']['BatchNorm']))\n",
        "            else:\n",
        "                prev_out = layers[-1][2] if layers else 0\n",
        "                layers.append((layer_type, prev_out, prev_out,\n",
        "                             config['colors'].get(layer_type, config['colors']['Other'])))\n",
        "\n",
        "        return layers\n",
        "\n",
        "    # Parse the model representation\n",
        "    layers = parse_model_repr(model_repr)\n",
        "\n",
        "    # Add input layer if needed\n",
        "    if not layers:\n",
        "        raise ValueError(\"Model appears to have no layers\")\n",
        "\n",
        "    # Always add input layer at beginning\n",
        "    layers.insert(0, (f\"Input\\n({layers[0][1]})\",\n",
        "                   layers[0][1],\n",
        "                   layers[0][1],\n",
        "                   config['colors']['Input']))\n",
        "\n",
        "    # Create figure and axis (same as before)\n",
        "    fig, ax = plt.subplots(figsize=config['figsize'])\n",
        "    ax.set_xlim(*config['x_limits'])\n",
        "    ax.set_ylim(*config['y_limits'])\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Draw layers\n",
        "    for i, (name, in_feat, out_feat, color) in enumerate(layers):\n",
        "        x_pos = i * config['layer_spacing'] + config['start_x']\n",
        "\n",
        "        # Draw the layer box with rounded corners\n",
        "        box = FancyBboxPatch((x_pos, config['layer_y'] - config['box_height']/2),\n",
        "                            config['box_width'], config['box_height'],\n",
        "                            boxstyle=f\"round,pad={config['box_padding']},rounding_size={config['box_rounding']}\",\n",
        "                            linewidth=config['box_linewidth'],\n",
        "                            edgecolor='#333333',\n",
        "                            facecolor=color)\n",
        "        ax.add_patch(box)\n",
        "\n",
        "        # Add layer name\n",
        "        ax.text(x_pos + config['box_width']/2, config['layer_y'], name,\n",
        "                ha='center', va='center', fontsize=config['fontsize'])\n",
        "\n",
        "        # Draw arrows between layers\n",
        "        if i > 0:\n",
        "            prev_x = (i-1) * config['layer_spacing'] + config['start_x'] + config['box_width'] + config['box_padding']\n",
        "            curr_x = x_pos - config['box_padding']\n",
        "            arrow = Arrow(prev_x, config['layer_y'], curr_x - prev_x, 0,\n",
        "                         width=config['arrow_width'], color=config['arrow_color'], alpha=config['arrow_alpha'])\n",
        "            ax.add_patch(arrow)\n",
        "\n",
        "    # Draw grouping boxes for Linear->ReLU pairs\n",
        "    group_indices = []\n",
        "    for i in range(len(layers)-1):\n",
        "        if 'Linear' in layers[i][0] and 'ReLU' in layers[i+1][0]:\n",
        "            group_indices.append((i, i+1))\n",
        "\n",
        "    for start_idx, end_idx in group_indices:\n",
        "        x_start = start_idx * config['layer_spacing'] + config['start_x']\n",
        "        x_end = end_idx * config['layer_spacing'] + config['start_x'] + config['box_width']\n",
        "        group_width = x_end - x_start\n",
        "        group_box = FancyBboxPatch((x_start, config['layer_y'] - config['box_height']*0.9),\n",
        "                                 group_width, config['group_box_height'],\n",
        "                                 boxstyle=f\"round,pad={config['group_box_padding']},rounding_size={config['group_box_rounding']}\",\n",
        "                                 linewidth=config['group_box_linewidth'],\n",
        "                                 linestyle='--',\n",
        "                                 edgecolor='#555555',\n",
        "                                 facecolor='none',\n",
        "                                 alpha=config['group_box_alpha'])\n",
        "        ax.add_patch(group_box)\n",
        "\n",
        "    # Add legend\n",
        "    legend_elements = [\n",
        "        Rectangle((0,0),1,1, fc=config['colors']['Input'], label='Input'),\n",
        "        Rectangle((0,0),1,1, fc=config['colors']['Linear'], label='Linear'),\n",
        "        Rectangle((0,0),1,1, fc=config['colors']['ReLU'], label='ReLU'),\n",
        "        Rectangle((0,0),1,1, fc=config['colors']['BatchNorm'], label='BatchNorm'),\n",
        "        Rectangle((0,0),1,1, fc=config['colors']['Output'], label='Output'),\n",
        "        Rectangle((0,0),1,1, fc=config['colors']['Other'], label='Other'),\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='upper right',\n",
        "              bbox_to_anchor=(1, 1), fontsize=9, framealpha=0.9)\n",
        "\n",
        "    plt.title(config['title'], fontsize=config['title_fontsize'], pad=20, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_repr = \"\"\"\n",
        "MLPModel(\n",
        "  (model): Sequential(\n",
        "    (0): Linear(in_features=47, out_features=32, bias=True)\n",
        "    (1): ReLU()\n",
        "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
        "    (3): Softplus(beta=1.0, threshold=20.0)\n",
        "  )\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "config = {'arrow_width': 0.1}\n",
        "visualize_mlp(model_repr, config)"
      ],
      "metadata": {
        "id": "1qlN7qE24blk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}