{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ubaidillah-chem/fouling-ml/blob/main/01_dataset_compiler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Mount your drive"
      ],
      "metadata": {
        "id": "bz5U4uX4T94-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "lTIXkklDuPpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API and Viscosity calculator\n",
        "\n",
        "def calc_api_and_viscosities(rho, watson_k):\n",
        "  SG = rho / 999\n",
        "  api = 141.5 / SG - 131.5\n",
        "\n",
        "  if api > 4.78231 * watson_k - 50.3642:\n",
        "    log_nu_100 = (\n",
        "          4.39371\n",
        "          - 1.94733 * watson_k\n",
        "          + 0.12769 * watson_k**2\n",
        "          + 0.00032629 * api**2\n",
        "          - 0.0118246 * watson_k * api\n",
        "          + ((0.1716 * watson_k**2 + 10.9943 * api + 0.0950663 * api**2 - 0.860218 * watson_k * api)\n",
        "            / (api + 50.3642 - 4.78231 * watson_k))\n",
        "      )\n",
        "    log_nu_100 = min(log_nu_100, 6)\n",
        "  else:\n",
        "    log_nu_100 = 6\n",
        "  nu_100 = 10**log_nu_100\n",
        "\n",
        "  if api > 2.6296 * watson_k - 26.786:\n",
        "    log_nu_210 = (\n",
        "          -0.463634\n",
        "          - 0.166532 * api\n",
        "          + 0.000513447 * api**2\n",
        "          - 0.00848995 * watson_k * api\n",
        "          + ((0.080325 * watson_k + 1.24899 * api + 0.19768 * api**2)\n",
        "            / (api + 26.786 - 2.6296 * watson_k))\n",
        "      )\n",
        "    log_nu_210 = min(log_nu_210, 6)\n",
        "  else:\n",
        "    log_nu_210 = 6\n",
        "  nu_210 = 10**log_nu_210\n",
        "\n",
        "  return api, nu_100, nu_210\n",
        "\n",
        "# Load XGBOOST solubility model\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "model = xgb.Booster()\n",
        "model.load_model(\"/content/drive/MyDrive/xgboost_solubility_model.json\")\n",
        "\n",
        "def predict_solubility(MW, weighted_avg_nbp, temp):\n",
        "  press = 410 * 0.21\n",
        "  input_data = np.array([[MW, weighted_avg_nbp, temp, press]])\n",
        "  dmatrix = xgb.DMatrix(input_data)\n",
        "  C_O2 = model.predict(dmatrix)[0]\n",
        "  return C_O2\n"
      ],
      "metadata": {
        "id": "pSvngWzBu_R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Make sure you have the excel file in My Drive and compile the dataset"
      ],
      "metadata": {
        "id": "RMkTJroHUCnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "sara_df = pd.read_excel(\"drive/MyDrive/Asomaning, 1997.xlsx\", sheet_name=\"SARA and Elemental\")\n",
        "prop_df = pd.read_excel(\"drive/MyDrive/Asomaning, 1997.xlsx\", sheet_name=\"Bulk Properties\").iloc[1:]\n",
        "prop_df[['API', 'Viscosity @ 100F', 'Viscosity @ 210F']] = prop_df.apply(lambda row: calc_api_and_viscosities(row['Density @ 60F'], row['Watson K']), axis=1, result_type='expand')\n",
        "\n",
        "oil_props = []\n",
        "for idx, row in prop_df.iterrows():\n",
        "  if idx == 0:\n",
        "    continue\n",
        "  oil_type = row['Oil Type']\n",
        "  prop = row.iloc[1:].to_numpy(dtype='float64')\n",
        "  sara = sara_df[sara_df['Oil Type'] == oil_type].iloc[0, 1:].to_numpy(dtype='float64')\n",
        "\n",
        "  oil_df = pd.read_excel(\"drive/MyDrive/Asomaning, 1997.xlsx\", sheet_name=oil_type, skiprows=3).drop(columns=['Viscosity 1', 'Viscosity 2'])\n",
        "  oil_df = oil_df.rename(columns={'Density': 'Density @ 60F'}).iloc[1:]\n",
        "  oil_df[['API', 'Viscosity @ 100F', 'Viscosity @ 210F']] = oil_df.apply(lambda row: calc_api_and_viscosities(row['Density @ 60F'], row['Watson K']), axis=1, result_type='expand')\n",
        "  oil_pcs = oil_df.iloc[:, 1:].to_numpy(dtype='float64').flatten()\n",
        "\n",
        "  weighted_avg_nbp = (oil_df['NBP'] * oil_df['Molar Compositions ']).sum()\n",
        "  oil_props.append([oil_type] + prop.tolist() + [weighted_avg_nbp] + sara.tolist() + oil_pcs.tolist())\n",
        "\n",
        "sara_labels = sara_df.columns.to_list()[1:]\n",
        "prop_labels = prop_df.columns.to_list()[1:]\n",
        "pc_props = oil_df.columns.to_list()[1:]\n",
        "pcs_labels = [f\"PC{i}_{label}\" for i in range(len(oil_df)) for label in pc_props]\n",
        "oil_props = pd.DataFrame(oil_props, columns=['Oil Type'] + prop_labels + ['Weighted Avg NBP'] + sara_labels + pcs_labels)\n",
        "\n",
        "test_matrix_df = pd.read_excel(\"drive/MyDrive/Asomaning, 1997.xlsx\", sheet_name='Test Matrix').drop(columns=['Remarks   '])\n",
        "runs = test_matrix_df['Run #  '].to_list()\n",
        "test_matrix = []\n",
        "for idx, row in test_matrix_df.iterrows():\n",
        "  run = row['Run #  ']\n",
        "  oil_type = row['Oil type']\n",
        "  oil_prop = oil_props[oil_props['Oil Type'] == oil_type].iloc[0, 1:]\n",
        "  thermo_data = row.iloc[2:].to_numpy(dtype='float64')\n",
        "\n",
        "  test_matrix.append(np.concatenate(([run], oil_prop, thermo_data)))\n",
        "\n",
        "oil_props_labels = oil_props.columns.to_list()[1:]\n",
        "thermo_labels = test_matrix_df.columns.to_list()[2:]\n",
        "test_matrix = pd.DataFrame(test_matrix, columns=['Run #'] + oil_props_labels + thermo_labels)\n",
        "\n",
        "O2solubilities = test_matrix.apply(lambda row: predict_solubility(row['Mol Wt.'], row['Weighted Avg NBP'], row['Average Tb (Â°C) ']), axis=1)\n",
        "test_matrix['DO (ppm)'] = O2solubilities * 32 * 1000 / test_matrix['Density @ 85C'] * test_matrix['Saturated with oxygen?']\n",
        "test_matrix = test_matrix.drop(columns=['Saturated with oxygen?'])\n",
        "\n",
        "dataset = []\n",
        "for sheet in ['Time-Rf', 'Time-Rf2']:\n",
        "  time_rf = pd.read_excel(\"drive/MyDrive/Asomaning, 1997.xlsx\", sheet_name=sheet)\n",
        "  runs = time_rf.columns.to_list()[1:]\n",
        "  for run in runs:\n",
        "    run_rf = time_rf[['Time', run]].dropna().to_numpy(dtype='float64')\n",
        "    oper_data = test_matrix[test_matrix['Run #'] == run].iloc[0, :]\n",
        "    oper_data_repeat = np.repeat([oper_data], np.shape(run_rf)[0], axis=0)\n",
        "    dataset.append(np.concatenate((oper_data_repeat, run_rf), axis=1))\n",
        "\n",
        "test_matrix_labels = test_matrix.columns.to_list()\n",
        "dataset_with_run_num = pd.DataFrame(np.concatenate(dataset), columns=test_matrix_labels + ['Time', 'Rf'])\n",
        "dataset = dataset_with_run_num.drop(columns=['Run #'])\n"
      ],
      "metadata": {
        "id": "NLB11DQvZbn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuring no N/A"
      ],
      "metadata": {
        "id": "ofYtehZSURbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.isna().sum().sum()"
      ],
      "metadata": {
        "id": "dBRBEEv5oe3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe you want to see the dataset..."
      ],
      "metadata": {
        "id": "t64X9mn3UbSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "Yl3Bnks1UZfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_with_run_num[dataset_with_run_num['Run #'] == 101]"
      ],
      "metadata": {
        "id": "NduSyNq3d6oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the dataset as a csv file directly to My Drive"
      ],
      "metadata": {
        "id": "pJhBzdo2UU3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_with_run_num.to_csv(\"drive/MyDrive/dataset_with_run_num.csv\", index=False)"
      ],
      "metadata": {
        "id": "tBn1q9dRbArg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.to_csv(\"drive/MyDrive/dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "cvSZRqBausNO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}